---
title: "DATA 607 Final Project"
author: "Team ADM"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
  pdf_document: default
---

# Part 1 - Introduction

One of the many wonderful aspects of the United States of America is its
diversity of people and culture. Historically, outside of major port and
immigration centers, cultures tend to be clustered geographically. This will
manifest not only in genetic differences, but also in differences in diet and
activity. A question can now be raised if there is a relationship between
geographic location, as measured by region, division, or state, and the death
rate (per 100,000) due to disease, possibly over time? In this research project
we will focus on heart disease.

```{r setup, message=FALSE, collapse=TRUE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE, message = FALSE)

# --- SET RANDOM SEED FOR REPRODUCIBILITY --- 
random_seed <- 15
set.seed(random_seed)

#  --- LOAD LIBRARIES ---
library(recipes)
library(caret)
library(rpart.plot)
library(jsonlite)
library(MASS)
library(tidyverse)
library(ggthemes)
library(glmnet)
library(data.table)
library(rsample)
library(corrr)

# Will be used for US Maps
library(maps)
us_states <- map_data("state")

# Will be used to build our Deep Learning model
library(tensorflow)
library(keras)
use_session_with_seed(random_seed, disable_gpu = TRUE, disable_parallel_cpu = TRUE, quiet = FALSE)

#' Attempt to load cached data falling back to remote URL loading
#'
#' To save redownloading data from the internet every time, this 
#' function will cache URL data the first time and for all future
#' references, it will us the locally cached file rather than
#' redownloading from the internet.
#' @param fn The local cache filename (character)
#' @param url The remote URL to load the data (character)
#' @param type The type of data (affects which loading fuction). Valid: csv, json (character)
#' @examples
#' myData <- load_cache_file('./data/myfile.csv', 'http://someurl.com/somefile.csv')
#' @return The desired data as a data.frame
#' @export
load_cache_file <- function(fn, url, type) {
  # TODO - Add error chacking of parameters (fn, url and type)
  
  if (!(file.exists(fn))) {
    if (type == 'csv') {
      data <- fread(url) 
    } else if (type == 'json') {
      data <- fromJSON(url, flatten = TRUE)
    }

    write.csv(data, file = fn, row.names = FALSE)
  } else {
    data <- fread(fn)
  }
  
  return(data)
}


#' Print a side-by-side Histogram and QQPlot of Residuals
#'
#' @param model A model
#' @examples
#' residPlot(myModel)
#' @return null
#' @export
residPlot <- function(model) {
  par(mfrow = c(1, 2))
  hist(model[[3]], freq = FALSE, breaks = "fd", main = "Residual Histogram",
       xlab = "Residuals")
  lines(density(model[[3]], kernel = "ep"))
  qqnorm(model[[3]], main = "Residual Q-Q plot")
  qqline(model[[3]])
  par(mfrow = c(1, 1))
}


#' Calcualte the R-Squared given observed and predicted data
#'
#' @param y     Observed values in an object that supports vector operations
#' @param y_hat Predicted values in an object that supports vector operations
#' @examples
#' r_squared(test_labels, predicted_labels)
#' @return Float between 0 and 1.0
#' @export
r_squared <- function(y, y_hat) {
  return(1 - sum((y - y_hat)^2) / sum((y - mean(y))^2))
}

```

# Part 2 - Data

### Load raw data

```{r loadData}

# --- Load LCD Data ---
# Note: we keep an original copy of the data in case we need to reference pre-modified data
LCD_fn <- './data/lcd.csv'
LCD_url <- 'https://data.cdc.gov/resource/bi63-dtpu.json?$limit=50000'
LCD_orig <- load_cache_file(LCD_fn, LCD_url, 'json')

LCD <- LCD_orig

# --- Load 2018 All data ---
CD1_fn <- './data/nst-est2018-alldata.csv'
CD1_url <- "https://www2.census.gov/programs-surveys/popest/datasets/2010-2018/national/totals/nst-est2018-alldata.csv"
CD1_orig <- load_cache_file(CD1_fn, CD1_url, 'csv')

CD1 <- CD1_orig

# --- Load 2010 State Age/Sex Data ---
CD2_fn <- './data/st-est00int-agesex.csv'
CD2_url <- "https://www2.census.gov/programs-surveys/popest/datasets/2000-2010/intercensal/state/st-est00int-agesex.csv"
CD2_orig <- load_cache_file(CD2_fn, CD2_url, 'csv')

CD2 <- CD2_orig
```

### Prepare Data Frames

```{r ETL}
# --- CLEAN LCD DATA ---
# fromJSON doesn't allow defining column classes on import
names(LCD) <- c("Year", "113 Cause Name", "Cause Name" ,"State" ,"Deaths" ,
                "Age-adjusted Death Rate")

LCD$Year <- as.integer(LCD$Year)
LCD$Deaths <- as.integer(LCD$Deaths)
LCD$`Age-adjusted Death Rate` <- as.double(LCD$`Age-adjusted Death Rate`)
setDT(LCD)

# --- EXTRACT CD1 DATA ---
C1Extract <- CD1[, c("NAME", paste0("POPESTIMATE", 2010:2017))]
setkey(C1Extract, "NAME")

# --- EXTRACT CD2 DATA ---
C2Extract <- CD2[SEX == 0L & AGE == 999L,
                 c("NAME",  paste0("POPESTIMATE", 2000:2009))]
setkey(C2Extract, "NAME")

# --- CREATE POPULATION DATAFRAME ---
Pop <- C2Extract[C1Extract, on = "NAME"]
setnames(Pop, names(Pop), c("State", 2000:2017))

Population <- melt(Pop, id.vars = "State", variable.name = "Year",
                   value.name = "Population", variable.factor = FALSE)
Population$Year <- as.integer(Population$Year)

# --- PREPARE HEART DISEASE DATAFRAME ---
DT <- Population[LCD[Year > 1999L], on = c("State == State", "Year == Year")]
SD <- data.table(State = state.name,
                 Region = state.region,
                 Division = state.division)
DT <- SD[DT, on = "State"]
setkey(DT, Year, State)

DT[, `DeathRate` := Deaths / Population * 1e5]
DTHD <- DT[`Cause Name` == "Heart disease" &
              !(State %chin% c("United States",
                               "District of Columbia",
                               "Puerto Rico"))]

head(DTHD)
```

## Source

The investigation will be based on publicly available data on leading causes
of death comes from the
[National Center for Health Statistics (NHCS)](https://data.cdc.gov/NCHS/NCHS-Leading-Causes-of-Death-United-States/bi63-dtpu).
Population data will be based on the statewide census data for both
[2010--2018](https://www.census.gov/data/tables/time-series/demo/popest/2010s-state-total.html)
and [2000-2009](https://www.census.gov/data/datasets/time-series/demo/popest/intercensal-2000-2010-state.html) from the US Census Bureau.

## ETL

The dataset from the CDC contains `r dim(LCD)[[1]]` observations comprising data
from 53 locations---the 50 states, the US as a whole, and the District of
Columbia---across 11 causes of death---the top 10 together with all---over the
years `r min(LCD$Year)`--`r max(LCD$Year)`. In order to perform aggregations,
the data will be joined with US statewide population estimates for those years.
Focusing on heart disease, there will be 50 location observations (the US should
be a sum of the rest and DC will be ignored) across the
`r diff(range(LCD$Year))` years for investigated cases of heart disease.

As the *Age-adjusted death rate* depends on unseen age-cohort populations, there
is no accurate way to calculate division or regional aggregate rates without
that information, the pure rate-per-100K people, called *DeathRate* will be used
as the dependent variable.

The NHCS data will be downloaded via their API as a JSON file and converted to
a data frame. The census data will be downloaded directly as CSV files and also
converted to data frames. The population data, especially that for 2000-2010,
requires tidying. Both data sets need to have the population estimate fields
extracted from the larger collection of fields.

The earlier data needs more work. It is also split by age group and sex, but
includes the data for "all" as well. Therefore, knowing that all sexes is
coded as `0` and all locations as `999`, only rows with those features are
extracted, but those columns themselves are not.

Once the population fields are extracted, the two census databases are joined to
each other using the "NAME" field---the geographic location---as the index. At
this point the data is in "wide" format, and needs to be converted to "long"
format, especially as geographic divisions and regions need to be added.
Therefore, the data is melted into long format. This data is now joined to a
database of US regions and divisions. This becomes the master disease table.
The heart-diseases specific information is extracted to its own table for ease
of analysis.

# Part 3 - Exploratory data analysis

## Summary Statistics

Summary statistics for the heart-disease data by division and region are below:

```{r sumStat}
options(digits = 4L)

DTHD[, .(Mean = mean(DeathRate),
         SD = sd(DeathRate),
         Min = min(DeathRate),
         FirstQ = quantile(DeathRate, 0.25),
         Median = median(DeathRate),
         ThirdQ = quantile(DeathRate, 0.75),
         Max = max(DeathRate),
         IQR = IQR(DeathRate)), by = Region]

DTHD[, .(Mean = mean(DeathRate),
         SD = sd(DeathRate),
         Min = min(DeathRate),
         FirstQ = quantile(DeathRate, 0.25),
         Median = median(DeathRate),
         ThirdQ = quantile(DeathRate, 0.75),
         Max = max(DeathRate),
         IQR = IQR(DeathRate)), by = Division]
```

## State View

A graphical view of the overall kernel-smoothed density and average rate over
time is shown below. It is faceted by state, colored by division, and for the
trend chart, the line-type indicates the region.

```{r graphs1, fig.height=9.5, fig.width=14}

# We need to merge/join state data to build ggplot maps
us_states$region <- str_to_title(us_states$region)
DTHD_states <- left_join(us_states, DTHD, by = c("region" = "State"))

# Display Deathrate on US Map
ggplot(data = DTHD_states,
       mapping = aes(x = long, y = lat, group = group, fill = DeathRate)) +
  geom_polygon(color = "gray30", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  scale_fill_gradient(low = "white", high = "#CB454A") +
  labs(title = "Death Rates") + 
  theme_map() + 
  labs(fill = "Death rate per 100,000 population",
       title = "Heart Related Deaths by State, 2000-2017") +
  theme(plot.title = element_text(size = 20, face = "bold", hjust = 0.5))

# Display Deathrates by Year on US Map
ggplot(data = subset(DTHD_states, Year > 1999),
       mapping = aes(x = long, y = lat, group = group, fill = DeathRate)) +
  geom_polygon(color = "gray30", size = 0.05) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  scale_fill_gradient(low = "white", high = "#CB454A") +
  theme_map() + 
  facet_wrap(~ Year, ncol = 4) +
    theme(legend.position = "bottom", strip.background = element_blank()) +
    labs(fill = "Death rate per 100,000 population",
         title = "Heart Related Deaths by State, 2000-2017") +
  theme(plot.title = element_text(size = 20, face = "bold", hjust = 0.5)) 

# Display ordered Boxplots by state to visualize best/worst states by DeathRate
ggplot(DTHD, aes(x = reorder(State, DeathRate, median),
                 y = DeathRate, color = Region)) + 
  geom_boxplot() +
  geom_hline(yintercept = median(DTHD$DeathRate), color = "gray60") +
  coord_flip() +
  labs(y = "Death rate per 100,000 population", x = "State",
       title = "Heart Related Deaths by State, 2000-2017") +
  theme(plot.title = element_text(size = 20, face = "bold", hjust = 0.5))

# Display Deathrate density plots by state
ggplot(DTHD, aes(x = DeathRate)) +
  stat_density(aes(fill = Division), bw = "nrd0") + facet_wrap(~State)

# Display Deathrate over time by state
ggplot(DTHD, aes(x = Year, y = DeathRate)) +
  geom_path(aes(color = Division, linetype = Region)) + facet_wrap(~State)
```

## Regional View

As striking as the displays may be, treating each state as its own factor would
fall prey to having too many variables and not enough data. Looking at the data
as a boxplot by Region would be better.

```{r boxPlot}

# Add a column with Region mean DeathRate
DTHD_regions <- DTHD %>% 
  group_by(Region) %>% 
  mutate(RegionMedian = median(DeathRate, na.rm = TRUE))
DTHD_regions <- left_join(us_states, DTHD_regions, by = c("region" = "State"))

# Display Regions on US Map
ggplot(data = DTHD_regions,
       mapping = aes(x = long, y = lat, group = group, fill = Region)) +
  geom_polygon(color = "gray30", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  labs(title = "Regions") + 
  theme_map() + 
  labs(fill = "Regions")

# Display Mean Deathrate for each Region on US Map
ggplot(data = DTHD_regions,
       mapping = aes(x = long, y = lat, group = group, fill = RegionMedian)) +
  geom_polygon(color = "gray30", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  scale_fill_gradient(low = "green", high = "red") +
  labs(title = "Regional Death Rates") + 
  theme_map() + 
  labs(fill = "Regional Death Rate")

# Boxplot of DeathRat by Region
ggplot(DTHD, aes(x = Region, y = DeathRate, color = Region)) + 
  geom_boxplot() +
  coord_flip()
```

The `West` region appears to be noticeably different from the other regions,
which provides us with some evidence that geographic differentiation may be
valuable. A second item of interest is that the `South` seems to have the most
variation.

The `DeathRate` variable is normalized to rate per 100K lives, so it can be
analyzed across years. However, it is prudent to look at the trend of the rate
over time as well.

```{r rateOverTime}
ggplot(DTHD, aes(x = Year, y = DeathRate, color = Region, group = Region)) +
  stat_summary(fun.data = "mean_se")
```

The rates all follow the same pattern, decreasing until about 2010 and then
increasing. As there is no fundamentally different pattern between regions, the
inference will begin with them aggregated. Note the singularity of the `West`
region and wider spread of `South` is visible in this chart as well. Lastly,
while more difficult to see, now that the trends were made apparent, they can
be followed in the succession of boxplots by year below.

```{r boxPlots, fig.height=9.5, fig.width=14}
ggplot(DTHD, aes(x = Region, y = DeathRate, color = Region)) + geom_boxplot() +
  coord_flip() + facet_wrap( ~ Year)
```

## Divisional View

A similar exploration of dividing the data by the nine divisions, instead of the
four regions, follows below.

```{r divisions}

# Add a column with Division mean DeathRate
DTHD_divisions <- DTHD %>% 
  group_by(Division) %>% 
  mutate(DivisionMedian = median(DeathRate, na.rm = TRUE))
DTHD_divisions <- left_join(us_states, DTHD_divisions, by = c("region" = "State"))

# Display Regions on US Map
ggplot(data = DTHD_divisions,
       mapping = aes(x = long, y = lat, group = group, fill = Division)) +
  geom_polygon(color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  labs(title = "Regions") + 
  theme_map() + 
  labs(fill = "Regions")

# Display Mean Deathrate for each Division on US Map
ggplot(data = DTHD_divisions,
       mapping = aes(x = long, y = lat, group = group, fill = DivisionMedian)) +
  geom_polygon(color = "gray30", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
  scale_fill_gradient(low = "green", high = "red") + 
  labs(title = "Regional Death Rates") + 
  theme_map() + 
  labs(fill = "Regional Death Rate")

# Boxplot 
ggplot(DTHD, aes(x = Division, y = DeathRate, color = Division)) +
  geom_boxplot() + coord_flip()

# DeathRate over time
ggplot(DTHD, aes(x = Year, y = DeathRate, color = Division, group = Division)) +
  stat_smooth(method = "loess", se = FALSE, aes(lty = Region)) +
  scale_color_brewer(type = 'qual', palette = 'Set1')
```

For the boxplots, the divisions are colored by region for identification
purposes. For the mean over time, it is easier to digest if the divisions are
differently colored and the line type reflects the regions. Here, most of the
divisions follow the same path over time, with the possible exception of the
`Eath South Central` division in the `South` region and the `Mountain` division
of the `West` region. Nevertheless, it does not immediately prevent us from
looking at all years simultaneously. It is also apparent that there is enough
inter-regional difference that a model using divisions would likely perform
better than one using solely regions.

# Part 4 - Inference

## Linear Models

As we are not privy to the underlying data from the NHCS, just their aggregated
data, we are restricted in the models we may build. Furthermore, the point of
this observational study is not to create a model of future deaths for 
prediction purposes. Rather, it is to demonstrate that the use of the
geographical variables is significant. There clearly is some relationship
between time rate, which is also clearly nonlinear. To show the benefit of
geography, a comparison between a model solely dependent on `Year` and then one
with added geography will be made. If geography is a valuable indicator, the
new model should show increased \(R^2\) and reduced error.

### Year

As seen above, the relationship between `DeathRate` and `Year` is non-linear.
One option is to add a quadratic term which will capture the curvature. Of
course, if extended too far, the quadratic term would be ludicrous, but it may
be valuable for short-term prediction. A second option would be to add a
cut-off. The simplest would be to pick a year based on the empirical evidence,
and treat that as a factor. A more sophisticated third method would put in dummy
variables for each year and test the cutoff that way.

```{r year1}
# Run Linear Regression using Year to predict DeathRate
year1_lm <- lm(DeathRate ~ Year, data = DTHD)

(yr1_lm_s <- summary(year1_lm))

residPlot(yr1_lm_s)
```

As expected, looking solely at `Year` a linear model works very poorly. Yes, 
`Year` is clearly significant based on the F-statistics and t-statistics, but
the amount of variability explained is very low as measured by the adjusted
\(R^2\) of `r yr1_lm_s$adj.r.squared`. The residuals are basically normally
distributed, and the QQ plot, while not perfect, is not egregious.

Next is the quadratic year model:

```{r yearquad}
# Run Linear Regression using Year and quadratic Year Term
year2_lm <- lm(DeathRate ~ Year + I(Year ^ 2), data = DTHD)

(yr2_lm_s <- summary(year2_lm))

residPlot(yr2_lm_s)
```

This is a somewhat better model, based on the F-statistics, t-statistics, and
slightly more variability explained is very low as measured by the adjusted
\(R^2\) of `r yr2_lm_s$adj.r.squared`. The residual behavior is similar to that
of the linear model although the QQ plot looks a little worse.

However, note what happened to the intercept. Due to the presence of `Year` and
`Year ^ 2`, the intercept is now three orders of magnitude above the average
deaths rates. While this performs better locally, a few years out and the
quadratic age factor will overwhelm the model making it near useless.

The next step is to replace the year with cutoffs and possibly "spline" the
results out of linear pieces.

```{r year_multi}
# Add logical columns to help with identifying year cutoffs
for (i in seq(min(LCD$Year), max(LCD$Year))) {
  DTHD[, paste0("Post", i) := Year > i]
}

yr_mult_formula <- reformulate(
  termlabels = c(paste0("Post", seq(min(LCD$Year) + 1L, max(LCD$Year) - 1L))),
  response = "DeathRate")

# Run Linear Regression 
yr_mult_lm <- lm(yr_mult_formula, data = DTHD)

yr_mult_lm_final <- stepAIC(yr_mult_lm, direction = "both",
                            scope = list(upper = yr_mult_lm, lower = ~ 1),
                             scale = 0, trace = FALSE)

(yr_lmf_s <- summary(yr_mult_lm_final))

residPlot(yr_lmf_s)
```

The `stepAIC` function from the `MASS` package was used to select the "best"
model using maximum likelihood, starting with the saturated model including all
years as cutoffs except the first and the last. This model is nearly identical
to the quadratic year model when measured by its adjusted \(R^2\) of
`r yr_lmf_s$adj.r.squared`. It captures the curvature by identifying "elbows" at
2003, 2005, and 2008, and the curve upward at 2014. However, it is getting more
difficult to say that the residuals exhibit normal behavior. This stands to
reason as it is becoming quite clear that a linear model does not capture all
the information buried in the data.

As neither of the "acceptable" models show an adjusted \(R^2\) greater than 
`r yr_lmf_s$adj.r.squared`, if geography increases this significantly, it
would indicate that our hypothesis is valid.

### Regions

The first step would be to add `Regions` to the "best" model using only `Year`.
Instead, `Region` will be added to the saturated cut-off model and then the
`stepAIC` procedure will be applied. While not explicitly looking for
interactions, this may allow for a better selection of years based on region.

Adding regions naively would select the `Northeast` region as the base. This
would be problematic as two of the other regions are close to it but `West` is
not. The intercept would be affected and may make the deviations from base for
the other regions less relevant. Re-leveling the data so that `West` is
considered the baseline will make the findings more relevant. The two simple
models using region only below show this difference.

```{r lm_region}
# Linear regression  - use Region to predict DeathRate
region1_lm <- lm(DeathRate ~ Region, data = DTHD)

(r1_lm_s <- summary(region1_lm))

# Change the reference level to the West Region and rerun the Linear Regression
DTHD$Region <- relevel(DTHD$Region, ref = "West")
region2_lm <- lm(DeathRate ~ Region, data = DTHD)

(r2_lm_s <- summary(region2_lm))
```

Note that the adjusted \(R^2\) remains the same at `r r1_lm_s$adj.r.squared`,
but the parameters are more significant in their distance from `West` than from
`Northeast`.

Now for the addition of `Region` to `Year`:

```{r regionyr1}
rg_mult_formula <- reformulate(
  termlabels = c('Region', paste0("Post", seq(min(LCD$Year) + 1L,
                                              max(LCD$Year) - 1L))),
  response = "DeathRate")
rg_mult_lm <- lm(rg_mult_formula, data = DTHD)
rg_mult_lm_final <- stepAIC(rg_mult_lm, direction = "both",
                            scope = list(upper = rg_mult_lm, lower = ~ 1),
                             scale = 0, trace = FALSE)
rg_lmf_s <- summary(rg_mult_lm_final)
rg_lmf_s
residPlot(rg_lmf_s)
```

The model is clearly significant as can be seen from the F and t statistics.
There is also a marked improvement in the adjusted \(R^2\) of the model at
`r rg_lmf_s$adj.r.squared`. Moreover, the residuals look better than the
year-only model as does the QQ-plot.

### Divisions

The next step is to subdivide on divisions. For the same reason as above, the
data will be re-leveled to consider the `Pacific` region as the base.

```{R lm_division}
DTHD$Division <- relevel(DTHD$Division, ref = "Pacific")
div_mult_formula <- reformulate(
  termlabels = c('Division', paste0("Post", seq(min(LCD$Year) + 1L,
                                              max(LCD$Year) - 1L))),
  response = "DeathRate")
div_mult_lm <- lm(div_mult_formula, data = DTHD)
div_mult_lm_final <- stepAIC(div_mult_lm, direction = "both",
                            scope = list(upper = div_mult_lm, lower = ~ 1),
                             scale = 0, trace = FALSE)
div_lmf_s <- summary(div_mult_lm_final)
div_lmf_s
residPlot(div_lmf_s)
```

This is an even better model in that the adjusted \(R^2\) has risen to
`r div_lmf_s$adj.r.squared`, which is more than half of the variation explained.
However, the residuals show some lumpiness in the lower tail, which is more
clearly seen in the QQ-plot. While not proof positive of a problem it is
indicative of the model being sub-optimal. Which is once again to be expected.
The object here is not to create the best predictive model of heart
disease-related deaths, but to indicate if geography plays a role.

### Penalized Regression

When dealing with many possible variables, as these annual dummy variables are,
one can use the machinery of lasso. Lasso, or more properly known as penalized
regression using L1, is a kind of regression that penalizes multiple parameters
with a linear penalty. Its relative, ridge regression, uses a quadratic penalty.
The benefit of lasso is that by having the penalty be sub-quadratic, variable
selection occurs as well. With quadratic penalties, the variable weights may
approach 0 asymptotically, but never hit it.

Lasso regression in R is usually carried out by the `glmnet` package, written by
the originators of the method, Hastie and Tibshirani. It doesn't use the simple
formula method that base R models use, but requires a model matrix. The
`model.matrix` function in base (`stats`) R can usually turn most formula
objects into matrices.

```{r lasso, fig.width=10, fig.height=8}
MM <- model.matrix(div_mult_lm)
YY <- DTHD$DeathRate
fit_lasso <- glmnet(MM, YY, family = 'gaussian', alpha = 1)
fit_lasso
r2_bestStep <- which(abs(fit_lasso$dev.ratio - div_lmf_s$adj.r.squared) ==
                    min(abs(fit_lasso$dev.ratio - div_lmf_s$adj.r.squared)))
plot(fit_lasso, label = TRUE)
```

Unfortunately, this hasn't helped much. The saturated model can explain at most
`r max(fit_lasso$dev.ratio)` of the variance, and to explain more than one-third
of the deviance, lasso requires 8 non-zero coefficients. The model which
explains about `r div_lmf_s$adj.r.squared` of the deviance---the value from the
best regional and year-cutoff model---is model number `r r2_bestStep`, which has
a lambda penalty of `r fit_lasso$lambda[[r2_bestStep]]` and
`r fit_lasso$df[[r2_bestStep]]` coefficients which are:

```{r lambdacoef}
coef(fit_lasso, s = fit_lasso$lambda[[r2_bestStep]])
```

This is very close, but less parsimonious, than the 13-parameter stepAIC model
above.

One other interesting note is that of the behavior of the `Mountain` division,
which as the ninth variable in the sequence above is depicted by the number 9.
The `glmnet` parameter plot shows that the all the "significant" parameters,
depending on the penalty, stay as positive or negative when compared to the
baseline. The very first "significant" parameter---number 9---is different. It
starts negative, and becomes even more strongly negative until around 15
parameters, at which point it slopes upwards. As the penalty changes and the
saturated model is approached, the coefficient actually becomes positive.

## Tree-based method

It is clear that the true relationship between `DeathRate` and the geographies,
even when accounting for the year, is non-linear. The linear model has, however,
shown that the geographies are relevant. We can show this with a tree-based
model as well. Moreover, a tree-based model will give us an idea of the relative
importance of a variables by its distance from the root. As we are not trying to
create a new predictive model but to identify characteristics of the observed
data, **all** the data will be used for training. Lastly, since a tree-based
model answers a yes-no question at each node, there is no need to use the dummy
variables indicating pre- or post- a given year---the `Year` variable may be
trained on directly.

```{r tree, fig.width=10, fig.height=8}
dt_m <- caret::train(DeathRate ~ Division + Year, data = DTHD, tuneLength = 19L,
              method = 'rpart2')
rpart.plot(dt_m$finalModel, type = 1L)
```

As surmised from the penalized regression result, the `Mountain` division
behaves differently enough to be the first choice in the selected model. Then
**2006** is selected as the key differentiator---not 2010 as was considered
above. Perhaps that is when the downward trend ended and the average between 
2006 and 2017 is basically flat. Following is `East South Central` and
`Middle Atlantic`, the former of which was also noticed in the exploratory
section. Lastly, it is interesting to note that prior to 2006 (the right side of
the tree) the `East South Central` and `Middle Atlantic` divisions are different
enough to split off directly. All other divisions depend on whether or not it
is after 2003. Prior to 2003 they all end in the same bucket. This is reinforced
by the stepAIC model which also found `Post2003` and `Post2005` to be highly
significant. This is also reinforced by the Lasso model above. Looking at the
variable importance plots, the most important was variable 9, `Mountain`,
followed by variables 13, 15, and 5 which are `Post2003`, `Post2005`, and
`East South Central` respectively. Not in the exact same order as the tree,
but close enough for adding confidence.

## Population

It would be intersting to see whether the state Popultion has any direct 
correlation with DeathRate.  We could hypothesize that higher state 
populations might lead to better healthcare (decrease deathrate) or greater
stress (increase deathrate).  Conversly, lower state populations might lead
to better living (decrease deathrate) and/or less accessible healthcare 
(increase deathreate).  At this point we have NO way to attribute *causality*,
but we can explore where there are *correlations* between Population and 
DeathRate.  Since Population is a longtail, we `log` transform Population 
in the model.

```{r pop1}
hist(DTHD$Population)
hist(log(DTHD$Population), breaks = 15)

# Run Linear Regression using Population to predict DeathRate
pop_lm <- lm(DeathRate ~ log(Population), data = DTHD)

(pop_lm <- summary(pop_lm))

residPlot(pop_lm)

# Calcuate correlation between Population and DeathRate
DTHD %>% 
  dplyr::select(Population, DeathRate) %>% 
  correlate(method = "spearman", diagonal = 1) %>%
  focus(Population)
```

Interestingly, while there is a significant positive relationship between 
`log(Population)` and the `DeathRate` within each State, it only explains 
a very small percent of the variability with an adjusted \(R^2\) 
of `r pop_lm$adj.r.squared`.  The residuals are essentially normally distributed,
and the QQ plot, is quite straight.

## Deep Learning Model

While we aren't interested in a model to predict future values, it is interesting to
train a deep learning models to see whether it can gain further explanatory 
value from given features through non-linear combinations. For this evaluation, 
we hold out the 2017 year data and train the model with the years, 2000-2016.

We will include `State`, `Region`, `Division`, `Year` and `Population` as features and 
predict the outcome, `DeathRate`. Note that there is huge colinearity between
`State`, `Division` and `Region` which can be problematic with other ML algorithms, 
but Deep Learning models are mostly insensitive.  We include these additional 
features as surrounding geos may provide predictive value for a given state.  
In future analysis, we could explore dropping out `Region` and/or `Division`.

### Prepare Data

```{r keras_data}
# Add a column with mean Division DeathRate
DTHD_engineered <- DTHD

# Factors didn't work for recipes below - columns ned to be charater vectors
DTHD_engineered$Region <- as.character(DTHD_engineered$Region)
DTHD_engineered$Division <- as.character(DTHD_engineered$Division)

# Calcualte the mean DeathRate per Division and add this as a column
dm <- DTHD_engineered %>% 
  group_by(Division) %>% 
  mutate(DivisionMean = mean(DeathRate, na.rm=TRUE))

DTHD_engineered$DivisionMean <- dm$DivisionMean

# separate out training data
dthd.training <- DTHD_engineered %>%
  dplyr::select(State, Region, Division, Year, Population, DeathRate) %>%
  filter(Year < 2017)

# separeate out test data
dthd.test <- DTHD_engineered %>%
  dplyr::select(State, Region, Division, Year, Population, DeathRate) %>%
  filter(Year == 2017)

# Create recipe for transforming columns
rec_obj <- recipe(DeathRate ~ ., data = dthd.training) %>%
  step_log(Population) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  prep(data = dthd.training)

# Apply recipe to prepare data for input into training
train_data <- bake(rec_obj, new_data = dthd.training) %>% dplyr::select(-DeathRate)
test_data  <- bake(rec_obj, new_data = dthd.test) %>% dplyr::select(-DeathRate)

# Response variables for training and testing sets
train_labels <- dthd.training %>% dplyr::select(DeathRate, -Division) 
test_labels  <- dthd.test %>% dplyr::select(DeathRate, -Division)

# convert everything to matrices
train_data <- as.matrix(train_data)
dimnames(train_data) <- NULL

test_data <- as.matrix(test_data)
dimnames(test_data) <- NULL

train_labels <- as.matrix(train_labels)
dimnames(train_labels) <- NULL

test_labels <- as.matrix(test_labels)
dimnames(test_labels) <- NULL

```


### Define the Model

```{r keras_model}

#' Wrapper to build a Keras Model
#' 
#' @param input_nodes   integer - number of input features
#' @param output_nodes  integer - number of output nodes
#' @param FLAGS         list - hyperparameter options
#' @return model
#' @export
build_model <- function(input_nodes, output_nodes, FLAGS) {
  use_session_with_seed(random_seed, disable_gpu = TRUE, disable_parallel_cpu = TRUE, quiet = FALSE)

  # Build our Keras Model
  model <- keras_model_sequential() %>%
    layer_dense(units = FLAGS$neuron1, activation = 'relu', input_shape = list(input_nodes) ) %>%
    layer_dense(units = FLAGS$neuron2, activation = 'relu') %>%
    layer_dense(units = FLAGS$neuron3, activation = 'relu') %>%
    layer_dense(units = output_nodes)
  
  # compile the model for use
  model %>% compile(
    loss      = 'mae',  # "mse"
    optimizer = optimizer_rmsprop(), # optimizer_rmsprop(),  lr = 0.002
    metrics   = list("mean_absolute_error") #list("mean_absolute_error")
  )
  
  return(model)
}

# Display training progress by printing a single dot for each completed epoch.
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 80 == 0) cat("\n")
    cat(".")
  }
) 


# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)


```

### Train the model 

Note that we did a grid search of hyperparameters to optimize the model.  Also,
R-Squared is not normally a metric we calculate with models.  Typically, we
score models with Mean Absolute Error (MAE), RMSE, or even log versions of these.
We added a function to caluclate R-Squared for the model so we could directly 
compare performance with previous models in this project.

```{r keras_train}

# Store off the scores from each hypertuing loop for evaluation
scores <- list()
iteration <- 1

# Code used for hypertuning is commented out
#for (neuron1 in c(128, 256, 512)) {
#  for (neuron2 in c(32, 64, 128, 256)) {
#   for (neuron3 in c(32, 64, 128, 256)) {
#     for (lr in c(0.0001, 0.001, 0.01)) {
      
#       FLAGS <- list('neuron1'=neuron1, 'neuron2'=neuron2, 'lr'=lr)

        # These were the optimal hyperparameters we tested
        FLAGS <- list('neuron1' = 256, 'neuron2' = 32, 'neuron3' = 32, 'lr' = 0.001)
  
        # Build the model with provided parameters
        model <- build_model(ncol(train_data), 1, FLAGS)
        
        # Fit the model 
        history <- model %>% fit(
            train_data, 
            train_labels, 
            epochs = 500, 
            batch_size = 20, 
            validation_split = 0.2,
            verbose = 0,
            callbacks = list(print_dot_callback)
        )
  
        # Evaluate on test data and labels
        score <- model %>% evaluate(test_data, test_labels, batch_size = 128)
        
        # Calcualte R squared for both the training and test data so we can compare with other models
        train_r2 <- r_squared(train_labels, model %>% predict(train_data))
        test_r2 <- r_squared(test_labels, model %>% predict(test_data))
        
        scores[[iteration]] <- list("n1" = FLAGS$neuron1, 
                                    "n2" = FLAGS$neuron2, 
                                    "n3" = FLAGS$neuron3, 
                                    "lr" = FLAGS$lr, 
                                    "score" = score, 
                                    "trn_r2" = train_r2, 
                                    "test_r2" = test_r2)
        #print(scores[[iteration]])
        
        iteration <- iteration + 1
#    }
#  }
#}

```

Adjusting hyperparameters lead to models with R-squared from a low of ~0.3 to 
a high of ~`r mean(scores[[1]]$trn_r2, scores[[1]]$test_r2)`.  The optimal 
result of hypertuning led to a model with three hidden layers with 
`r FLAGS$neuron1`, `r FLAGS$neuron2`, and `r FLAGS$neuron3` hidden nodes respectively.
We found a learning rate of `r FLAGS$lr` to be optimal.  Note that with more time, 
we could have explored additional hidden layers, dropout layers, etc. to further 
refine our model.  

Our R-squared on the training data was **`r scores[[1]]$trn_r2`** and on 
the holdout test data, **`r scores[[1]]$test_r2`**.  

When training models, there is variability.  In general we expect the training 
scores to be slightly higher than test scores as a model memorizes and overfits 
to the training data.  As long as the training score and test score are close, 
we are fine and the the model is generalizing.  If the training score were 
much higher then we might suspect overfitting, and conversely, if the test 
score were much higher, we might suspect information leakage.  

Note that we set this up as a timeseries, training on the 2000-2016 data and 
validated with the 2017 data.  This was intentional ... we wanted the model 
to pick up on trends within the data to better understand how to interpret 
the `Year` feature.

### Evaluate Model 

```{r keras_eval}

# Plot the accuracy of the training data 
plot(history, metrics = "mean_absolute_error", smooth = FALSE) 

# Evaluate on test data and labels
(score <- model %>% evaluate(test_data, test_labels, batch_size = 128))

# Calcualte R squared for both the training and test data so we can compare with other models
r_squared(train_labels, model %>% predict(train_data))
r_squared(test_labels, model %>% predict(test_data))
```

# Part 5 - Conclusion
There is a statistically significant relationship between geographic location in
the United States and the straight death rate per 100K people due to heart
disease. This would indicate that it would be worthwhile to investigate
differences in diet, behavior, genetics, and culture in the different US
divisions to perhaps create more fine-tuned and efficient treatments and
prevention efforts.

The rate has been changing over time as well. A model comprising merely
division and time, while clearly significant, cannot explain more than around
60% of the variance from the general mean. A richer feature set would prove
valuable.

Lastly, attention should be paid to the `Mountain` and `East South Central`
divisions as their behavior in the models differs from the other divisions.

# References
* [NCHS - Leading Causes of Death: United States](https://data.cdc.gov/NCHS/NCHS-Leading-Causes-of-Death-United-States/bi63-dtpu)
* [State Population Totals and Components of Change: 2010--2018](https://www.census.gov/data/tables/time-series/demo/popest/2010s-state-total.html)
* [Intercensal Estimates of the Resident Population by Single Year of Age and Sex for States and the United States: April 1, 2000 to July 1, 2010](https://www.census.gov/data/datasets/time-series/demo/popest/intercensal-2000-2010-state.html)
* Friedman, J., Hastie, T. and Tibshirani, R. (2008) *Regularization Paths for Generalized Linear Models via Coordinate Descent*, https://web.stanford.edu/~hastie/Papers/glmnet.pdf
* Breiman L., Friedman J. H., Olshen R. A., and Stone, C. J. (1984) *Classification and Regression Trees*. Wadsworth.
